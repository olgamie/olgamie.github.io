<!DOCTYPE html>
<html lang="en-us">
    <head>
         
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Improve random forest performance using variable importance in R</title>
        
        <style>

    html body {
        font-family: 'Roboto', sans-serif;
        background-color: white;
    }

    :root {
        --accent: #28d102;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="/css/main.css">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/solarized-dark.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
 


    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

     <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script> 

    <script>hljs.initHighlightingOnLoad();</script>







<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.54.0" />
        

        
        
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-138037459-1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments)};
          gtag('js', new Date());

          gtag('config', 'UA-138037459-1');
        </script>
        
    </head>

    
    
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    

    <body>
         
        <nav class="navbar navbar-default navbar-fixed-top">

            <div class="container">

                <div class="navbar-header">

                    <a class="navbar-brand visible-xs" href="#">Improve random forest performance using variable importance in R</a>

                    <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>

                </div>

                <div class="collapse navbar-collapse">

                    
                        <ul class="nav navbar-nav">
                            
                                <li><a href="/">Home</a></li>
                            
                                <li><a href="/post">Blog</a></li>
                            
                                <li><a href="/about">About</a></li>
                            
                        </ul>
                    

                    
                        <ul class="nav navbar-nav navbar-right">
                            
                                <li class="navbar-icon"><a href="mailto:olga.mierzwa@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://github.com/olgamie"><i class="fa fa-github"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://twitter.com/olga_mie/"><i class="fa fa-twitter"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://www.linkedin.com/in/olga-mierzwa-sulima-84843539/"><i class="fa fa-linkedin"></i></a></li>
                            
                        </ul>
                    

                </div>

            </div>

        </nav>


<main>

    <div class="item">

    
    
    

    
    

    <h4><a href="/2020/06/15/improve-random-forest-performance-using-variable-importance-in-r/">Improve random forest performance using variable importance in R</a></h4>
    <h5>June 15, 2020</h5>
    
    <a href="/tags/random-forest"><kbd class="item-tag">random forest</kbd></a>
    
    <a href="/tags/variable-importance"><kbd class="item-tag">variable importance</kbd></a>
    
    <a href="/tags/dalex"><kbd class="item-tag">DALEX</kbd></a>
    
    <a href="/tags/r"><kbd class="item-tag">R</kbd></a>
    

</div>


    <br> <div class="text-justify">
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="tltr" class="section level1">
<h1>tltr</h1>
<p>I fine-tune a random forest (RF) model using variable importance in R.
I show how to use <code>DALEX</code> package to understand a random forest built with <code>ranger</code> package, simplify it, improve performance and run time. I use <code>recipes</code> package to handel categorical variables, missing values and novel levels.</p>
<div id="variable-importance" class="section level2">
<h2>Variable importance</h2>
<p>Feature or variable importance let‚Äôs you understand which features are important in a given machine learning model. Usually there is only a handful of variables that are important and you want to care about when building and understanding a model.</p>
<p>The main idea behind variable importance is to measure how much the model fit decreases if the effect of a variable is removed. The effect is removed by permuting the column. Values still have the same distribution but it‚Äôs relation to the dependent variable is gone. If a variable is important, then after random shuffling the model performance should decrease. The larger drop in the performance, the more important is the variable. Read more about pros and cons of this approach in the <a href="https://pbiecek.github.io/ema/featureImportance.html">Explanatory Model Analysis. Explore, Explain and Examine Predictive Models e-book</a>.</p>
<p><code>DALEX</code> package in R offers this powerful and model agnostic way of calculating variable importance.</p>
<div id="how-does-it-affect-model-performance" class="section level3">
<h3>How does it affect model performance?</h3>
<p>Dropping unimportant columns should not decrease the model performance. By removing a bunch of columns we can get rid of the source of collinearity between the features. The importance that previously was split between the columns should only be reflected in the one left in the model. This makes the feature importance values more clear and reliable.</p>
<p>For example random forest has less columns to try making a split, so it has a slightly better chance to make a better decisions. Model should take less time to run.</p>
<p>The biggest gain comes in less variables that a modeler needs to understand and worry in a model.</p>
</div>
</div>
<div id="analysis-approach" class="section level2">
<h2>Analysis approach</h2>
<p>I take an example from <a href="https://www.fast.ai/">fast.ai</a> Machine Learning course in Python and show how to do similar analysis using tools available in R.
I use Kaggle‚Äôs competition <a href="https://www.kaggle.com/c/bluebook-for-bulldozers/data">Blue Book for Bulldozers</a> data and predict an auction price for a heavy equipment with a random forest.
I build a model on a data which I pre-process in a very limited way using <code>recipes</code> package, just so <code>ranger</code> package can actually works.
After I have the model I plug it into <code>DALEX</code> explainer, perform variable importance shuffling and visualize the results. Next by interpreting the results I make adjustments to the model.</p>
<p>The main idea here is to build a RF model as soon as possible without thinking too much about the data and use model interpretation techniques to make it better.</p>
</div>
<div id="solution-in-r" class="section level2">
<h2>Solution in R</h2>
<p>Read in R packages.</p>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(ranger)
library(DALEX)
library(tidymodels)
library(rlang)</code></pre>
<p>Read in data.</p>
<pre class="r"><code>set.seed(123)
data_raw &lt;- read_csv(file = &quot;Train.csv&quot;)</code></pre>
<div id="data-pre-processing" class="section level3">
<h3>Data pre-processing</h3>
<p>There are few things to be done with the data before pluging them into the model.</p>
<ul>
<li>I need to handel missing data both in numeric and cardinal columns. In the numeric columns missing values are replaced with median with a <code>recipe</code> step <code>step_medianimpute()</code>. I add a dummy column for every numeric column that contains NAs using <code>add_dummy_for_na_numeric()</code> that I define myself.
I replace NAs in character columns with ‚ÄúNA‚Äù string and threat missing values as a separate level with own <code>replace_na_in_char_col()</code>.</li>
<li>I make sure my cardinal variables enter the model as factors. I use <code>recipe</code> <code>step_novel()</code> to make sure model can handel unseen levels in the validation set.</li>
<li>I add new features based on saledate column such as: elapsed number of days since 1 Jan 1970, day, week, month, year, day of week, day of year and indicators is year/quarter/month beginning/end using <code>proc_date()</code>.</li>
</ul>
<pre class="r"><code>add_dummy_for_na_numeric &lt;- function(x) {
  x %&gt;%
    mutate_if(~any(is.na(.)) &amp; is.numeric(.),
              .funs = list(na = ~if_else(is.na(.), 1, 0)))
}

replace_na_in_char_col &lt;- function(x) {
  x %&gt;% 
    mutate_if(is_character, ~if_else(is.na(.), &quot;NA&quot;, .))
}

proc_date &lt;- function(data, var_name) {
  data %&gt;%
    dplyr::mutate(
      elapsed = floor(unclass(as.POSIXct({{var_name}}))/86400),
      year = lubridate::year({{var_name}}),
      month = lubridate::month({{var_name}}),
      day = lubridate::day({{var_name}}),
      week = lubridate::week({{var_name}}),
      day_of_week = lubridate::wday({{var_name}}),
      day_of_year = lubridate::yday({{var_name}}),
      is_end_of_month =
        if_else({{var_name}} == (ceiling_date({{var_name}}, unit = &quot;month&quot;) - 1),
                TRUE, FALSE),
      is_begining_of_month =
        if_else({{var_name}} == floor_date({{var_name}}, unit = &quot;month&quot;),
                TRUE, FALSE),
      is_end_of_quarter =
        if_else({{var_name}} == (ceiling_date({{var_name}},
                                              unit = &quot;quarter&quot;) - 1),
                TRUE, FALSE),
      is_begining_of_quarter =
        if_else({{var_name}} == floor_date({{var_name}}, unit = &quot;quarter&quot;),
                TRUE, FALSE),
      is_end_of_year = 
        if_else({{var_name}} == (ceiling_date({{var_name}}, unit = &quot;year&quot;) - 1),
                TRUE, FALSE),
      is_begining_of_year =
        if_else({{var_name}} == floor_date({{var_name}}, unit = &quot;year&quot;),
                TRUE, FALSE),
      saledate = NULL
    )
    
}</code></pre>
<p>Pre-process data by calling helper functions.</p>
<pre class="r"><code>data_after_basic_preprocess &lt;- data_raw %&gt;%
  dplyr::mutate(
    saledate = lubridate::as_date(saledate, format = &quot;%m/%d/%Y %H:%M&quot;, tz = &quot;GMT&quot;)
    ) %&gt;%
  proc_date(saledate) %&gt;% 
  replace_na_in_char_col() %&gt;% 
  add_dummy_for_na_numeric()</code></pre>
<p>I take the log of <code>SalePrice</code>. It makes the variable normally distributed and changes the relative distance between numbers.</p>
<pre class="r"><code>data_step1 &lt;- data_after_basic_preprocess %&gt;%
  mutate(log_SalePrice = log(SalePrice),
         SalePrice = NULL)</code></pre>
<p>I Split data into training and validation set using <code>initial_time_split()</code> function from <code>rsample</code> package. I keep 80% of data in the training set and 20% in the validation.</p>
<pre class="r"><code>data_step1_split &lt;- initial_time_split(data = data_step1, prop = 0.80)
train_part &lt;- training(data_step1_split)
valid_part &lt;- testing(data_step1_split)</code></pre>
<p>Define <code>recipes</code> steps to finalize pre-processing required for the model.</p>
<pre class="r"><code>recipe_formula &lt;-  recipe(log_SalePrice ~ ., data = data_step1_split)
                          
recipe_steps &lt;- recipe_formula %&gt;%
  step_novel(all_nominal()) %&gt;% 
  step_medianimpute(all_numeric()) %&gt;%
  step_string2factor(all_nominal())

recipe_steps_on_training &lt;- recipe_steps %&gt;% 
  prep(train_part)</code></pre>
<p>Now let‚Äôs <code>juice()</code> and <code>bake()</code> to create training and validation sets and split <code>x</code> and <code>y</code> for <code>ranger</code> model to avoid using formula.</p>
<pre class="r"><code>training_set &lt;- recipe_steps_on_training %&gt;%
  juice()

y_train &lt;- training_set$log_SalePrice
x_train &lt;- training_set %&gt;% select(-log_SalePrice)


validation_set &lt;- recipe_steps_on_training %&gt;%
  bake(valid_part)

x_valid &lt;- validation_set %&gt;% select(-log_SalePrice)
y_valid &lt;- validation_set$log_SalePrice</code></pre>
</div>
<div id="model-building" class="section level3">
<h3>Model building</h3>
<p>I build random forest using <code>ranger</code> package. I‚Äôm not using <code>randomForest</code>, because it has very slow implementation in R.
I limit the number of trees to speed the training process to 40.</p>
<p>To avoid over-fitting:</p>
<ul>
<li>use a subset of columns when building trees by setting <code>mtry</code> parameter to build less correlated trees,</li>
<li>use a subset of rows when building trees by setting <code>sample.fraction</code>, can be used to build less correlated trees and speed up training,</li>
<li>stop training the tree further when a leaf node has 3 or less samples. Numbers that usually works well 1, 3, 5, 10, 25.</li>
</ul>
<pre class="r"><code>system.time(ranger_model_with_sampling &lt;- ranger(x = x_train,
                       y = y_train,
                       num.trees = 40,
                       mtry = (ncol(x_train) / 2), 
                       seed = 1,
                       min.node.size = 3,
                       sample.fraction = 0.4))</code></pre>
<pre><code>## Growing trees.. Progress: 18%. Estimated remaining time: 2 minutes, 45 seconds.
## Growing trees.. Progress: 33%. Estimated remaining time: 2 minutes, 31 seconds.
## Growing trees.. Progress: 53%. Estimated remaining time: 1 minute, 38 seconds.
## Growing trees.. Progress: 73%. Estimated remaining time: 55 seconds.
## Growing trees.. Progress: 88%. Estimated remaining time: 25 seconds.</code></pre>
<pre><code>## u≈ºytkownik     system   up≈Çynƒô≈Ço 
##     306.00       2.67     208.65</code></pre>
<pre class="r"><code>ranger_predictions &lt;- predict(ranger_model_with_sampling, x_valid)

rmse &lt;- function(preds, y){
  sqrt(mean((preds - y)**2))
}
eval_model &lt;- function(model, y, y_valid, model_predictions) {
  c(model$r.squared,
      rmse(model$predictions, y),
      rmse(model_predictions, y_valid)
  )
}

eval_model(
  ranger_model_with_sampling, y_train, y_valid, ranger_predictions$predictions)</code></pre>
<pre><code>## [1] 0.9103894 0.2073261 0.2932974</code></pre>
</div>
<div id="model-interpretation" class="section level3">
<h3>Model interpretation</h3>
<p>Now when I have my random forest built I can try to understand it and look at variable importance plot. I use <code>DALEX</code> package and create <code>ranger</code> explainer. <code>DALEX</code> package provides tones of useful tools for understanding black box models.</p>
<pre class="r"><code>explainer_ranger  &lt;- explain(ranger_model_with_sampling,
                             data = x_train,
                             y = y_train)</code></pre>
<pre><code>## Preparation of a new explainer is initiated
##   -&gt; model label       :  ranger  ( [33m default [39m )
##   -&gt; data              :  320900  rows  66  cols 
##   -&gt; data              :  tibbble converted into a data.frame 
##   -&gt; target variable   :  320900  values 
##   -&gt; model_info        :  package ranger , ver. 0.12.1 , task regression ( [33m default [39m ) 
##   -&gt; predict function  :  yhat.ranger  will be used ( [33m default [39m )
##   -&gt; predicted values  :  numerical, min =  8.526752 , mean =  10.1027 , max =  11.83357  
##   -&gt; residual function :  difference between y and yhat ( [33m default [39m )
##   -&gt; residuals         :  numerical, min =  -1.775955 , mean =  -0.0001989848 , max =  1.353764  
##  [32m A new explainer has been created! [39m</code></pre>
<p>I use <code>variable_importance()</code> function from <code>ingredients</code> (DALEX family) package to calculate the variable importance. My loss function is RMSE. DALEX performs by default 10 permutation rounds. I will average the permutation results and inspect the plot for the best 30 features (for better plot readability).
I drop <code>_baseline_</code> from the plot. Baseline is the change in model performance when all variables are permuted.</p>
<pre class="r"><code>var_importance &lt;- variable_importance(explainer_ranger,
                                      loss = loss_root_mean_square,
                                      B = 1,
                                      type = &quot;raw&quot;)

summarize_vi &lt;- var_importance %&gt;%
  group_by(variable) %&gt;% 
  summarize(mean_dropout_loss = mean(dropout_loss)) %&gt;%
  arrange(-mean_dropout_loss) %&gt;%
  mutate(variable = factor(variable, levels = variable))  

ggplot(data = summarize_vi[2:31,],
       aes(y = mean_dropout_loss, x = variable, group = 1)) +
  geom_line() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))</code></pre>
<p><img src="/post/2020-06-15-improve-rf-with-vi_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>There are only few variables that my random forest cares about. <code>YearMade</code> is the most important one.
So next I select the features before the curve flattens almost completely at around 0.150 mean dropout loss and build a model on the remaining features subset.</p>
<pre class="r"><code>var_to_keep &lt;-  summarize_vi %&gt;%
  filter(mean_dropout_loss &gt; 0.150 &amp; variable != &quot;_baseline_&quot;) %&gt;% 
  pull(variable) %&gt;%
  as.character()

x_after_var_importance &lt;- x_train %&gt;% select(var_to_keep)</code></pre>
<pre class="r"><code>system.time(ranger_model_after_vi &lt;- ranger(x = x_after_var_importance,
                       y = y_train,
                       num.trees = 40,
                       mtry = (ncol(x_after_var_importance) / 2),
                       seed = 1,
                       min.node.size = 3,
                       sample.fraction = 0.4))</code></pre>
<pre><code>## Growing trees.. Progress: 35%. Estimated remaining time: 57 seconds.
## Growing trees.. Progress: 70%. Estimated remaining time: 26 seconds.</code></pre>
<pre><code>## u≈ºytkownik     system   up≈Çynƒô≈Ço 
##     164.53       1.53      94.70</code></pre>
<pre class="r"><code>ranger_predictions_after_vi &lt;- predict(ranger_model_after_vi,
                                       x_valid %&gt;% select(var_to_keep))

eval_model(
  ranger_model_after_vi, y_train, y_valid, ranger_predictions_after_vi$predictions)</code></pre>
<pre><code>## [1] 0.9099574 0.2078253 0.2917446</code></pre>
<p>I end up with a simpler model with almost the same quality, differences in R^2 and RMSE are neglectable. I prefer to continue with model with less features which runs faster.</p>
<p>Let‚Äôs again look at variable importance plot and do more digging.</p>
<pre class="r"><code>explainer_ranger_after_vi  &lt;- explain(ranger_model_after_vi,
                                      data = x_after_var_importance,
                                      y = y_train)</code></pre>
<pre><code>## Preparation of a new explainer is initiated
##   -&gt; model label       :  ranger  ( [33m default [39m )
##   -&gt; data              :  320900  rows  30  cols 
##   -&gt; data              :  tibbble converted into a data.frame 
##   -&gt; target variable   :  320900  values 
##   -&gt; model_info        :  package ranger , ver. 0.12.1 , task regression ( [33m default [39m ) 
##   -&gt; predict function  :  yhat.ranger  will be used ( [33m default [39m )
##   -&gt; predicted values  :  numerical, min =  8.53282 , mean =  10.10281 , max =  11.83919  
##   -&gt; residual function :  difference between y and yhat ( [33m default [39m )
##   -&gt; residuals         :  numerical, min =  -1.70521 , mean =  -0.0003111864 , max =  1.408141  
##  [32m A new explainer has been created! [39m</code></pre>
<pre class="r"><code>var_importance_ranger_after_vi &lt;- variable_importance(
  explainer_ranger_after_vi,
  loss_function = loss_root_mean_square,
  B = 1,
  type = &quot;raw&quot;)

plot(var_importance_ranger_after_vi)</code></pre>
<p><img src="/post/2020-06-15-improve-rf-with-vi_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p><code>YearMade</code> is the most important feature for my model. It would be interesting to learn more about this variable.
Second thing that stands out is how important are SalesID and MachineId. I expect these are unique identifiers so it‚Äôs strange they are so important. So it‚Äôs another thing worth looking into.</p>
<pre class="r"><code>ggplot(data_step1, aes(x = YearMade)) + 
  geom_histogram(binwidth = 10)</code></pre>
<p><img src="/post/2020-06-15-improve-rf-with-vi_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>data_step1 %&gt;% group_by(YearMade) %&gt;% count()</code></pre>
<pre><code>## # A tibble: 72 x 2
## # Groups:   YearMade [72]
##    YearMade     n
##       &lt;dbl&gt; &lt;int&gt;
##  1     1000 38185
##  2     1919   127
##  3     1920    17
##  4     1937     1
##  5     1942     1
##  6     1947     1
##  7     1948     3
##  8     1949     1
##  9     1950     8
## 10     1951     7
## # ... with 62 more rows</code></pre>
<p>After plotting YearMade distribution I can see that sth weird is happening. There is 38185 entries with YearMade 1000. This is not possible, so I remove this data and use the sales after the War ended.</p>
<pre class="r"><code>data_step1$SalesID %&gt;% n_distinct()</code></pre>
<pre><code>## [1] 401125</code></pre>
<pre class="r"><code>data_step1$MachineID %&gt;% n_distinct()</code></pre>
<pre><code>## [1] 341027</code></pre>
<pre class="r"><code>data_step1 %&gt;% filter(MachineID == 2283592)</code></pre>
<pre><code>## # A tibble: 26 x 67
##    SalesID MachineID ModelID datasource auctioneerID YearMade MachineHoursCur~
##      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;
##  1 4319288   2283592    4579        172            1     2005                0
##  2 4319290   2283592    4579        172            1     2006              991
##  3 4319294   2283592    4579        172            1     2005             2201
##  4 4319295   2283592    4579        172            1     2005                0
##  5 4319296   2283592    4579        172            1     2004             3405
##  6 4319297   2283592    4579        172            1     2005             2589
##  7 4319298   2283592    4579        172            1     2002             3802
##  8 4319299   2283592    4579        172            1     2004             2693
##  9 4319309   2283592    4579        172            1     1000             2033
## 10 4319310   2283592    4579        172            1     2008             1098
## # ... with 16 more rows, and 60 more variables: UsageBand &lt;chr&gt;,
## #   fiModelDesc &lt;chr&gt;, fiBaseModel &lt;chr&gt;, fiSecondaryDesc &lt;chr&gt;,
## #   fiModelSeries &lt;chr&gt;, fiModelDescriptor &lt;chr&gt;, ProductSize &lt;chr&gt;,
## #   fiProductClassDesc &lt;chr&gt;, state &lt;chr&gt;, ProductGroup &lt;chr&gt;,
## #   ProductGroupDesc &lt;chr&gt;, Drive_System &lt;chr&gt;, Enclosure &lt;chr&gt;, Forks &lt;chr&gt;,
## #   Pad_Type &lt;chr&gt;, Ride_Control &lt;chr&gt;, Stick &lt;chr&gt;, Transmission &lt;chr&gt;,
## #   Turbocharged &lt;chr&gt;, Blade_Extension &lt;chr&gt;, Blade_Width &lt;chr&gt;,
## #   Enclosure_Type &lt;chr&gt;, Engine_Horsepower &lt;chr&gt;, Hydraulics &lt;chr&gt;,
## #   Pushblock &lt;chr&gt;, Ripper &lt;chr&gt;, Scarifier &lt;chr&gt;, Tip_Control &lt;chr&gt;,
## #   Tire_Size &lt;chr&gt;, Coupler &lt;chr&gt;, Coupler_System &lt;chr&gt;, Grouser_Tracks &lt;chr&gt;,
## #   Hydraulics_Flow &lt;chr&gt;, Track_Type &lt;chr&gt;, Undercarriage_Pad_Width &lt;chr&gt;,
## #   Stick_Length &lt;chr&gt;, Thumb &lt;chr&gt;, Pattern_Changer &lt;chr&gt;, Grouser_Type &lt;chr&gt;,
## #   Backhoe_Mounting &lt;chr&gt;, Blade_Type &lt;chr&gt;, Travel_Controls &lt;chr&gt;,
## #   Differential_Type &lt;chr&gt;, Steering_Controls &lt;chr&gt;, elapsed &lt;dbl&gt;,
## #   year &lt;dbl&gt;, month &lt;dbl&gt;, day &lt;int&gt;, week &lt;dbl&gt;, day_of_week &lt;dbl&gt;,
## #   day_of_year &lt;dbl&gt;, is_end_of_month &lt;lgl&gt;, is_begining_of_month &lt;lgl&gt;,
## #   is_end_of_quarter &lt;lgl&gt;, is_begining_of_quarter &lt;lgl&gt;,
## #   is_end_of_year &lt;lgl&gt;, is_begining_of_year &lt;lgl&gt;, auctioneerID_na &lt;dbl&gt;,
## #   MachineHoursCurrentMeter_na &lt;dbl&gt;, log_SalePrice &lt;dbl&gt;</code></pre>
<p>SalesID is a unique identifier that sorts sales from the oldest to the newest.
Kaggle‚Äôs description says MachineID is unique machine identifier and that machine can be sold multiple times. I expect that RF model is overfitting on these columns so I remove them from the dataset and see how it impacts the model.</p>
<pre class="r"><code>x_train &lt;- x_train %&gt;%
  mutate(YearMade = if_else(YearMade &gt;= 1950, YearMade, 1950)) %&gt;%
  select(var_to_keep) %&gt;%
  select(-c(SalesID, MachineID))

x_valid &lt;- x_valid %&gt;%
  mutate(YearMade = if_else(YearMade &gt;= 1950, YearMade, 1950)) %&gt;%
  select(var_to_keep) %&gt;%
  select(-c(SalesID, MachineID))</code></pre>
<pre class="r"><code>system.time(ranger_model_after_vi_data_clean &lt;- ranger(x = x_train,
                       y = y_train,
                       num.trees = 40,
                       mtry = (ncol(x_train) / 2),
                       seed = 1,
                       min.node.size = 3,
                       sample.fraction = 0.4))</code></pre>
<pre><code>## Growing trees.. Progress: 48%. Estimated remaining time: 34 seconds.
## Growing trees.. Progress: 95%. Estimated remaining time: 3 seconds.</code></pre>
<pre><code>## u≈ºytkownik     system   up≈Çynƒô≈Ço 
##     130.65       1.12      73.89</code></pre>
<pre class="r"><code>ranger_predictions_after_vi_clean &lt;- predict(ranger_model_after_vi_data_clean,
                                       x_valid)

eval_model(
  ranger_model_after_vi_data_clean, y_train, y_valid, ranger_predictions_after_vi_clean$predictions)</code></pre>
<pre><code>## [1] 0.9109139 0.2067185 0.2511735</code></pre>
<p>It seems cleaning YearMade and dropping IDs improved the model.</p>
</div>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>I manage to improve model‚Äôs RMSE on validation set from 0.28 to 0.25, reduce number of features and get first inisghts into which variables matter to the model.</p>
</div>
</div>
</div>

    
    

    

        <h4 class="page-header">Related</h4>

         <div class="item">

    
    
    

    
    

    <h4><a href="/2020/02/03/data-science-salary-survey-in-poland-2019/">Data Science Salary Survey in Poland 2019</a></h4>
    <h5>February 3, 2020</h5>
    
    <a href="/tags/data-science"><kbd class="item-tag">data-science</kbd></a>
    
    <a href="/tags/salary"><kbd class="item-tag">salary</kbd></a>
    
    <a href="/tags/r"><kbd class="item-tag">R</kbd></a>
    
    <a href="/tags/python"><kbd class="item-tag">python</kbd></a>
    

</div>
  <div class="item">

    
    
    

    
    

    <h4><a href="/2019/05/03/parental-leave-across-oecd-countries/">Parental leave across OECD countries</a></h4>
    <h5>May 3, 2019</h5>
    
    <a href="/tags/r"><kbd class="item-tag">R</kbd></a>
    
    <a href="/tags/plotly"><kbd class="item-tag">plotly</kbd></a>
    
    <a href="/tags/parental-leave"><kbd class="item-tag">parental-leave</kbd></a>
    

</div>
  <div class="item">

    
    
    

    
    

    <h4><a href="/2019/04/10/the-trendiest-baby-names-in-poland/">The trendiest baby names in Poland</a></h4>
    <h5>April 10, 2019</h5>
    
    <a href="/tags/r"><kbd class="item-tag">R</kbd></a>
    
    <a href="/tags/baby-names"><kbd class="item-tag">baby-names</kbd></a>
    

</div>
 

    

    

</main>

        <footer>

            <p class="copyright text-muted">Copyright ¬© 2019 Olga Mierzwa-Sulima</p>

        </footer>
       
    </body>

</html>

